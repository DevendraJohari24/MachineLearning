{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import nltk.classify.util, nltk.metrics\n",
    "from nltk import precision, recall\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify import DecisionTreeClassifier\n",
    "from nltk.corpus import CategorizedPlaintextCorpusReader\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk import precision\n",
    "import string\n",
    "from tabulate import tabulate         \n",
    "import itertools\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path='aclImdb/train'\n",
    "train_data=CategorizedPlaintextCorpusReader(train_path,r'(pos|neg)/.*\\.txt',cat_pattern=r'(pos|neg)/.*\\.txt')\n",
    "test_path='aclImdb/test'\n",
    "test_data=CategorizedPlaintextCorpusReader(test_path,r'(pos|neg)/.*\\.txt',cat_pattern=r'(pos|neg)/.*\\.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_train_id = train_data.fileids('neg')\n",
    "positive_train_id = train_data.fileids('pos')\n",
    "negative_test_id = test_data.fileids('neg')\n",
    "positive_test_id = test_data.fileids('pos')\n",
    "\n",
    "negative_train = [(word_feats(train_data.words(fileids=[f])), 'neg') for f in negative_train_id]\n",
    "positive_train = [(word_feats(train_data.words(fileids=[f])), 'pos') for f in positive_train_id]\n",
    "negative_test = [(word_feats(test_data.words(fileids=[f])), 'neg') for f in negative_test_id]\n",
    "positive_test = [(word_feats(test_data.words(fileids=[f])), 'pos') for f in positive_test_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = positive_train + negative_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = positive_test + negative_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive_classifier = NaiveBayesClassifier.train(train_data)\n",
    "refsets = collections.defaultdict(set)\n",
    "testsets_Naive = collections.defaultdict(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (text, label) in enumerate(test_data):\n",
    "        refsets[label].add(i)           \n",
    "        observed_Naive = Naive_classifier.classify(text)\n",
    "        testsets_Naive[observed_Naive].add(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive Classifier Model: 82.66%\n",
      "Precision of Positive Review of Naive Classifier Model: 86.71%\n",
      "Precision of Negative Review of Naive Classifier Model: 79.41%\n"
     ]
    }
   ],
   "source": [
    "accuracy = nltk.classify.util.accuracy(Naive_classifier, test_data)  \n",
    "print(\"Accuracy of Naive Classifier Model: %0.2f\" % (accuracy*100) + \"%\")\n",
    "positive_precision = precision(refsets['pos'], testsets_Naive['pos'])\n",
    "print(\"Precision of Positive Review of Naive Classifier Model: %0.2f\" % (positive_precision*100) + \"%\")\n",
    "positive_recall = recall(refsets['pos'], testsets_Naive['pos'])\n",
    "negative_precision = precision(refsets['neg'], testsets_Naive['neg'])\n",
    "print(\"Precision of Negative Review of Naive Classifier Model: %0.2f\" % (negative_precision*100) + \"%\")\n",
    "negative_recall = recall(refsets['neg'], testsets_Naive['neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   Avoid = True              neg : pos    =     97.0 : 1.0\n",
      "                    Boll = True              neg : pos    =     37.7 : 1.0\n",
      "                     Uwe = True              neg : pos    =     36.3 : 1.0\n",
      "                 stinker = True              neg : pos    =     28.1 : 1.0\n",
      "                   WORST = True              neg : pos    =     27.8 : 1.0\n",
      "                  Paulie = True              pos : neg    =     24.3 : 1.0\n",
      "               awfulness = True              neg : pos    =     23.7 : 1.0\n",
      "             excellently = True              pos : neg    =     22.2 : 1.0\n",
      "                  Capote = True              pos : neg    =     21.7 : 1.0\n",
      "             unwatchable = True              neg : pos    =     21.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "Naive_classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.classify.SklearnClassifier(LinearSVC(max_iter=100000))\n",
    "SVM_classifier = classifier.train(train_data)\n",
    "refsets = collections.defaultdict(set)\n",
    "SVM_testset = collections.defaultdict(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (text, label) in enumerate(test_data):\n",
    "        refsets[label].add(i)           \n",
    "        SVM_observe = classifier.classify(text)\n",
    "        SVM_testset[SVM_observe].add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM Model: 85.78%\n",
      "Precision of Positive Review of SVM Model: 50.00%\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "accuracy = nltk.classify.util.accuracy(classifier, test_data)  \n",
    "print(\"Accuracy of SVM Model: %0.2f\" % (accuracy*100) + \"%\")\n",
    "positive_precision = precision(refsets['pos'], SVM_testset['pos'])\n",
    "print(\"Precision of Positive Review of SVM Model: %0.2f\" % (positive_precision*100) + \"%\")\n",
    "positive_recall = recall(refsets['pos'], SVM_testset['pos'])\n",
    "negative_precision = precision(refsets['neg'], SVM_testset['neg'])\n",
    "print(negative_precision)\n",
    "negative_recall = recall(refsets['neg'], SVM_testset['neg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-35526aad8418>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtrain_negcutoff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_negfeats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtrain_poscutoff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_posfeats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrainfeats_Decision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_negfeats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrain_negcutoff\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtrain_posfeats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrain_poscutoff\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mDecisionTree_classifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainfeats_Decision\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mrefsets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "train_negcutoff = len(train_negfeats)*1/100\n",
    "train_poscutoff = len(train_posfeats)*1/100\n",
    "trainfeats_Decision = train_negfeats[:train_negcutoff] + train_posfeats[:train_poscutoff]\n",
    "DecisionTree_classifier = DecisionTreeClassifier.train(trainfeats_Decision)\n",
    "refsets = collections.defaultdict(set)\n",
    "testsets_Decision = collections.defaultdict(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (feats, label) in enumerate(testfeats):\n",
    "        refsets[label].add(i)           \n",
    "        observed_Decision = DecisionTree_classifier.classify(feats)\n",
    "        testsets_Decision[observed_Decision].add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy3 = nltk.classify.util.accuracy(DecisionTree_classifier, testfeats)  \n",
    "pos_precision3 = nltk.metrics.precision(refsets['pos'], testsets_Decision['pos'])\n",
    "pos_recall3 = nltk.metrics.recall(refsets['pos'], testsets_Decision['pos'])\n",
    "neg_precision3 = nltk.metrics.precision(refsets['neg'], testsets_Decision['neg'])\n",
    "neg_recall3 = nltk.metrics.recall(refsets['neg'], testsets_Decision['neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier_Decision(featx):\n",
    "    train_negids = train.fileids('neg')\n",
    "    train_posids = train.fileids('pos')\n",
    "    test_negids = test.fileids('neg')\n",
    "    test_posids = test.fileids('pos')\n",
    "    train_negfeats = [(featx(train.words(fileids=[f])), 'neg') for f in train_negids]\n",
    "    train_posfeats = [(featx(train.words(fileids=[f])), 'pos') for f in train_posids]\n",
    "    test_negfeats = [(featx(test.words(fileids=[f])), 'neg') for f in test_negids]\n",
    "    test_posfeats = [(featx(test.words(fileids=[f])), 'pos') for f in test_posids]\n",
    "    trainfeats = train_negfeats + train_posfeats\n",
    "    testfeats = test_negfeats + test_posfeats\n",
    "\n",
    "    train_negcutoff = len(train_negfeats)*1/100\n",
    "    train_poscutoff = len(train_posfeats)*1/100\n",
    "    trainfeats_Decision = train_negfeats[:train_negcutoff] + train_posfeats[:train_poscutoff]\n",
    "    DecisionTree_classifier = DecisionTreeClassifier.train(trainfeats_Decision)\n",
    "    refsets = collections.defaultdict(set)\n",
    "    testsets_Decision = collections.defaultdict(set)\n",
    "\n",
    "    for i, (feats, label) in enumerate(testfeats):\n",
    "            refsets[label].add(i)           \n",
    "            observed_Decision = DecisionTree_classifier.classify(feats)\n",
    "            testsets_Decision[observed_Decision].add(i)\n",
    "\n",
    "    accuracy3 = nltk.classify.util.accuracy(DecisionTree_classifier, testfeats)  \n",
    "    pos_precision3 = nltk.metrics.precision(refsets['pos'], testsets_Decision['pos'])\n",
    "    pos_recall3 = nltk.metrics.recall(refsets['pos'], testsets_Decision['pos'])\n",
    "    neg_precision3 = nltk.metrics.precision(refsets['neg'], testsets_Decision['neg'])\n",
    "    neg_recall3 = nltk.metrics.recall(refsets['neg'], testsets_Decision['neg'])\n",
    "\n",
    "    return(['DecisionTree',accuracy3,pos_precision3,pos_recall3,neg_precision3,neg_recall3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_word_feats(words, score_fn=BigramAssocMeasures.chi_sq, n=200):\n",
    "    words_nopunc = [word for word in words if word not in string.punctuation]\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words_nopunc)\n",
    "    bigrams = bigram_finder.nbest(score_fn, n)\n",
    "    return dict([(ngram, True) for ngram in itertools.chain(words_nopunc, bigrams)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2 = []\n",
    "table2.append(evaluate_classifier_Naive(bigram_word_feats))\n",
    "table2.append(evaluate_classifier_SVM(bigram_word_feats))\n",
    "table2.append(evaluate_classifier_Decision(bigram_word_feats))\n",
    " \n",
    "print('Bigram word features:')\n",
    "print(tabulate(table2, headers=[\"Classifier\",\"Accuracy\",\"Positive precision\", \"Positive recall\", \"Negative precision\", \"Negative recall\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
