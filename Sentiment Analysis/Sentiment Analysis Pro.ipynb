{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import nltk.classify.util, nltk.metrics\n",
    "from nltk import precision, recall\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify import DecisionTreeClassifier\n",
    "from nltk.corpus import CategorizedPlaintextCorpusReader\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk import precision\n",
    "import string\n",
    "from tabulate import tabulate         \n",
    "import itertools\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path='aclImdb/train'\n",
    "train_data=CategorizedPlaintextCorpusReader(train_path,r'(pos|neg)/.*\\.txt',cat_pattern=r'(pos|neg)/.*\\.txt')\n",
    "test_path='aclImdb/test'\n",
    "test_data=CategorizedPlaintextCorpusReader(test_path,r'(pos|neg)/.*\\.txt',cat_pattern=r'(pos|neg)/.*\\.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_train_id = train_data.fileids('neg')\n",
    "positive_train_id = train_data.fileids('pos')\n",
    "negative_test_id = test_data.fileids('neg')\n",
    "positive_test_id = test_data.fileids('pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_train = [(word_feats(train_data.words(fileids=[f])), 'neg') for f in negative_train_id]\n",
    "positive_train = [(word_feats(train_data.words(fileids=[f])), 'pos') for f in positive_train_id]\n",
    "negative_test = [(word_feats(test_data.words(fileids=[f])), 'neg') for f in negative_test_id]\n",
    "positive_test = [(word_feats(test_data.words(fileids=[f])), 'pos') for f in positive_test_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = positive_train + negative_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = positive_test + negative_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive_classifier = NaiveBayesClassifier.train(train_data)\n",
    "refsets = collections.defaultdict(set)\n",
    "testsets_Naive = collections.defaultdict(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (text, label) in enumerate(test_data):\n",
    "        refsets[label].add(i)           \n",
    "        observed_Naive = Naive_classifier.classify(text)\n",
    "        testsets_Naive[observed_Naive].add(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive Classifier Model: 82.66%\n",
      "Precision of Positive Review of Naive Classifier Model: 86.71%\n",
      "Precision of Negative Review of Naive Classifier Model: 79.41%\n"
     ]
    }
   ],
   "source": [
    "accuracy = nltk.classify.util.accuracy(Naive_classifier, test_data)  \n",
    "print(\"Accuracy of Naive Classifier Model: %0.2f\" % (accuracy*100) + \"%\")\n",
    "positive_precision = precision(refsets['pos'], testsets_Naive['pos'])\n",
    "print(\"Precision of Positive Review of Naive Classifier Model: %0.2f\" % (positive_precision*100) + \"%\")\n",
    "positive_recall = recall(refsets['pos'], testsets_Naive['pos'])\n",
    "negative_precision = precision(refsets['neg'], testsets_Naive['neg'])\n",
    "print(\"Precision of Negative Review of Naive Classifier Model: %0.2f\" % (negative_precision*100) + \"%\")\n",
    "negative_recall = recall(refsets['neg'], testsets_Naive['neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   Avoid = True              neg : pos    =     97.0 : 1.0\n",
      "                    Boll = True              neg : pos    =     37.7 : 1.0\n",
      "                     Uwe = True              neg : pos    =     36.3 : 1.0\n",
      "                 stinker = True              neg : pos    =     28.1 : 1.0\n",
      "                   WORST = True              neg : pos    =     27.8 : 1.0\n",
      "                  Paulie = True              pos : neg    =     24.3 : 1.0\n",
      "               awfulness = True              neg : pos    =     23.7 : 1.0\n",
      "             excellently = True              pos : neg    =     22.2 : 1.0\n",
      "                  Capote = True              pos : neg    =     21.7 : 1.0\n",
      "             unwatchable = True              neg : pos    =     21.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "Naive_classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.classify.SklearnClassifier(LinearSVC(max_iter=100000))\n",
    "SVM_classifier = classifier.train(train_data)\n",
    "refsets = collections.defaultdict(set)\n",
    "SVM_testset = collections.defaultdict(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (text, label) in enumerate(test_data):\n",
    "        refsets[label].add(i)           \n",
    "        SVM_observe = classifier.classify(text)\n",
    "        SVM_testset[SVM_observe].add(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM Model: 85.78%\n",
      "Precision of Positive Review of SVM Model: 86.22%\n",
      "Precision of Negative Review of SVM Model: 85.35%\n"
     ]
    }
   ],
   "source": [
    "accuracy = nltk.classify.util.accuracy(classifier, test_data)  \n",
    "print(\"Accuracy of SVM Model: %0.2f\" % (accuracy*100) + \"%\")\n",
    "positive_precision = precision(refsets['pos'], SVM_testset['pos'])\n",
    "print(\"Precision of Positive Review of SVM Model: %0.2f\" % (positive_precision*100) + \"%\")\n",
    "positive_recall = recall(refsets['pos'], SVM_testset['pos'])\n",
    "negative_precision = precision(refsets['neg'], SVM_testset['neg'])\n",
    "print(\"Precision of Negative Review of SVM Model: %0.2f\" % (negative_precision*100) + \"%\")\n",
    "negative_recall = recall(refsets['neg'], SVM_testset['neg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_train_cutoff = len(negative_train)*1/100\n",
    "positive_train_cutoff = len(positive_train)*1/100\n",
    "train_Decision = negative_train[:int(negative_train_cutoff)] + positive_train[:int(positive_train_cutoff)]\n",
    "DecisionTree_classifier = DecisionTreeClassifier.train(train_Decision)\n",
    "refsets = collections.defaultdict(set)\n",
    "Decision_Test = collections.defaultdict(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (text, label) in enumerate(test_data):\n",
    "        refsets[label].add(i)           \n",
    "        observed_Decision = DecisionTree_classifier.classify(text)\n",
    "        Decision_Test[observed_Decision].add(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree Model: 61.74%\n",
      "Precision of Positive Review of Decision Tree Model: 61.17%\n",
      "Precision of Negative Review of Decision Tree Model: 62.37%\n"
     ]
    }
   ],
   "source": [
    "accuracy = nltk.classify.util.accuracy(DecisionTree_classifier, test_data)  \n",
    "print(\"Accuracy of Decision Tree Model: %0.2f\" % (accuracy*100) + \"%\")\n",
    "positive_precision = precision(refsets['pos'], Decision_Test['pos'])\n",
    "print(\"Precision of Positive Review of Decision Tree Model: %0.2f\" % (positive_precision*100) + \"%\")\n",
    "positive_recall = recall(refsets['pos'], Decision_Test['pos'])\n",
    "negative_precision = precision(refsets['neg'], Decision_Test['neg'])\n",
    "print(\"Precision of Negative Review of Decision Tree Model: %0.2f\" % (negative_precision*100) + \"%\")\n",
    "negative_recall = recall(refsets['neg'], Decision_Test['neg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_word_feats(words, score_fn=BigramAssocMeasures.chi_sq, n=200):\n",
    "    words_nopunc = [word for word in words if word not in string.punctuation]\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words_nopunc)\n",
    "    bigrams = bigram_finder.nbest(score_fn, n)\n",
    "    return dict([(ngram, True) for ngram in itertools.chain(words_nopunc, bigrams)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path='aclImdb/train'\n",
    "train_data=CategorizedPlaintextCorpusReader(train_path,r'(pos|neg)/.*\\.txt',cat_pattern=r'(pos|neg)/.*\\.txt')\n",
    "test_path='aclImdb/test'\n",
    "test_data=CategorizedPlaintextCorpusReader(test_path,r'(pos|neg)/.*\\.txt',cat_pattern=r'(pos|neg)/.*\\.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_train_id = train_data.fileids('neg')\n",
    "positive_train_id = train_data.fileids('pos')\n",
    "negative_test_id = test_data.fileids('neg')\n",
    "positive_test_id = test_data.fileids('pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_train = [(bigram_word_feats(train_data.words(fileids=[f])), 'neg') for f in negative_train_id]\n",
    "positive_train = [(bigram_word_feats(train_data.words(fileids=[f])), 'pos') for f in positive_train_id]\n",
    "negative_test = [(bigram_word_feats(test_data.words(fileids=[f])), 'neg') for f in negative_test_id]\n",
    "positive_test = [(bigram_word_feats(test_data.words(fileids=[f])), 'pos') for f in positive_test_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = positive_train + negative_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = positive_test + negative_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive_classifier = NaiveBayesClassifier.train(train_data)\n",
    "refsets = collections.defaultdict(set)\n",
    "testsets_Naive = collections.defaultdict(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (text, label) in enumerate(test_data):\n",
    "        refsets[label].add(i)           \n",
    "        observed_Naive = Naive_classifier.classify(text)\n",
    "        testsets_Naive[observed_Naive].add(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive Classifier Model: 86.74%\n",
      "Precision of Positive Review of Naive Classifier Model: 90.40%\n",
      "Precision of Negative Review of Naive Classifier Model: 83.69%\n"
     ]
    }
   ],
   "source": [
    "accuracy = nltk.classify.util.accuracy(Naive_classifier, test_data)  \n",
    "print(\"Accuracy of Naive Classifier Model: %0.2f\" % (accuracy*100) + \"%\")\n",
    "positive_precision = precision(refsets['pos'], testsets_Naive['pos'])\n",
    "print(\"Precision of Positive Review of Naive Classifier Model: %0.2f\" % (positive_precision*100) + \"%\")\n",
    "positive_recall = recall(refsets['pos'], testsets_Naive['pos'])\n",
    "negative_precision = precision(refsets['neg'], testsets_Naive['neg'])\n",
    "print(\"Precision of Negative Review of Naive Classifier Model: %0.2f\" % (negative_precision*100) + \"%\")\n",
    "negative_recall = recall(refsets['neg'], testsets_Naive['neg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_train_cutoff = len(negative_train)*1/100\n",
    "positive_train_cutoff = len(positive_train)*1/100\n",
    "train_Decision = negative_train[:int(negative_train_cutoff)] + positive_train[:int(positive_train_cutoff)]\n",
    "DecisionTree_classifier = DecisionTreeClassifier.train(train_Decision)\n",
    "refsets = collections.defaultdict(set)\n",
    "Decision_Test = collections.defaultdict(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (text, label) in enumerate(test_data):\n",
    "        refsets[label].add(i)           \n",
    "        observed_Decision = DecisionTree_classifier.classify(text)\n",
    "        Decision_Test[observed_Decision].add(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree Model: 60.73%\n",
      "Precision of Positive Review of Decision Tree Model: 60.16%\n",
      "Precision of Negative Review of Decision Tree Model: 61.36%\n"
     ]
    }
   ],
   "source": [
    "accuracy = nltk.classify.util.accuracy(DecisionTree_classifier, test_data)  \n",
    "print(\"Accuracy of Decision Tree Model: %0.2f\" % (accuracy*100) + \"%\")\n",
    "positive_precision = precision(refsets['pos'], Decision_Test['pos'])\n",
    "print(\"Precision of Positive Review of Decision Tree Model: %0.2f\" % (positive_precision*100) + \"%\")\n",
    "positive_recall = recall(refsets['pos'], Decision_Test['pos'])\n",
    "negative_precision = precision(refsets['neg'], Decision_Test['neg'])\n",
    "print(\"Precision of Negative Review of Decision Tree Model: %0.2f\" % (negative_precision*100) + \"%\")\n",
    "negative_recall = recall(refsets['neg'], Decision_Test['neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
